#!/bin/bash

#########################################################
# 
# Platform: NCI Gadi HPC
# Description: run GATK Base Recalibrator over parallel tasks
# Details:
# 	Run GATK BaseRecalibrator in 30 parallel chunks per sample
# 	For tumour/normal, run time is ~ half for normal so best to split
# 	into 2 jobs. GATK4 does not multithread, but this each tasks needs 
# 	2 CPU worth of RAM ( or 1 CPU of broadwell nodes). Average recal 
#	time for one tumour split interval 
#	(60X) is ~20 mins, and ~ 10 mins for blood (30X). Use this to work 
#	out the expected walltime based on the number of tasks reported by 
#	the 'make_input'script, eg for 6 normal samples
# 	that is 6 X 32 = 192 tasks total, at 2 CPU per task = 384 cores which is 
# 	8 nodes. Given this is a small dataset, I would request 8 nodes for 20
# 	minutes (double the expected walltime as a buffer). For larger datasets, 
# 	I reduce the ratio of nodes per sample, eg for 80 blood samples, 30 nodes 
# 	yields ~ 32 mins time, and for 80 tumour samples, 30 nodes yields 65 mins 
#	walltime. For 16 cancer samples, 16 x 32 = 512 tasks, x 2 CPU = 1024 CPU 
#	or 21.3 nodes. Requesting 22 nodes at 40 mins walltime would be fine for 
#	this example dataset size. Efficiency will be slightly lower but given 
#	the time per interval is fairly constant and can't be ordered in a specific 
#	way based on exected walltime (I have tested this), reducing the nodes 
#	won't help much (a little, as the idle cores at eg 22 nodes for this dataset
#	will be 32 and at 11 nodes will be 16 idle cores. It should only run for 20 
#	mins so the SU cost of the 32 idle cores won't add too much cost to the job.
#
# Author: Cali Willet
# cali.willet@sydney.edu.au
# Date last modified: 14/10/2020
#
# If you use this script towards a publication, please acknowledge the
# Sydney Informatics Hub (or co-authorship, where appropriate).
#
# Suggested acknowledgement:
# The authors acknowledge the scientific and technical assistance 
# <or e.g. bioinformatics assistance of <PERSON>> of Sydney Informatics
# Hub and resources and services from the National Computational 
# Infrastructure (NCI), which is supported by the Australian Government
# with access facilitated by the University of Sydney.
# 
#########################################################     
 

#PBS -P <project>
#PBS -N bsv-R1-S7
#PBS -l walltime=01:00:00
#PBS -l ncpus=1120
#PBS -l mem=10200GB
#PBS -q normalbw
#PBS -W umask=022 
#PBS -l wd 
#PBS -o ./Logs/bsv-R1-S7.o
#PBS -e ./Logs/bsv-R1-S7.e
#PBS -l storage=<lstorage>
 
module load openmpi/4.0.2
module load nci-parallel/1.0.0

set -e

SCRIPT=./bsv-R1-S7_bqsr_recal.sh  
INPUTS=./Inputs/bqsr_recal.inputs # just a sample ID and interval list, not specific to round 1

round=<round>

mkdir -p ./BQSR_recal_tables ./BQSR_recal_tables/Round${round} ./GATK_logs/BQSR_round${round} ./Error_capture/Bootstrap_BQSR_round${round}

NCPUS=1


#########################################################
# Do not edit below this line  
#########################################################

if [[ $PBS_QUEUE =~ bw-exec ]]; then CPN=28; else CPN=48; fi
M=$(( CPN / NCPUS )) #tasks per node

sed "s|^|${SCRIPT} |" ${INPUTS} > ${PBS_JOBFS}/input-file

mpirun --np $((M * PBS_NCPUS / CPN)) \
        --map-by node:PE=${NCPUS} \
        nci-parallel \
        --verbose \
        --input-file ${PBS_JOBFS}/input-file
