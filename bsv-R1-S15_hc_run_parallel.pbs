#!/bin/bash

#Run GATK HC using scatter-gather method
#Runs gatk4_hc.sh tasks in parallel
#1 task performs HC for 1 interval
#1 task requires 1 CPU, 4GB mem
#Some require up to 8 GB so provide this allowance to GATK


#PBS -P ch81
#PBS -N bsv-R1-S16
#PBS -l walltime=03:00:00
#PBS -l ncpus=7200
#PBS -l mem=28650GB
#PBS -q normal
#PBS -W umask=022
#PBS -l wd
#PBS -o ./Logs/bsv-R1-S16.o
#PBS -e ./Logs/bsv-R1-S16.e
#PBS -lstorage=scratch/er01+scratch/ch81

module load openmpi/4.0.2
module load nci-parallel/1.0.0

set -e

round=1

SCRIPT=./bsv-R1-S16_hc.sh
INPUTS=./Inputs/gatk4_hc.inputs

mkdir -p ./GVCFs ./GVCFs/Round${round} ./GATK_logs/HC_round${round}

NCPUS=1 #GATK4 does not multithread

#########################################################
# Do not edit below this line (unless running on a node that 
# does not have 48 CPU, in which case edit the value of 'CPN'
#########################################################

CPN=48 #CPUs per node.  48 for intel xeon, 28 for broadwell  
M=$(( CPN / NCPUS )) #tasks per node

sed "s|^|${SCRIPT} |" ${INPUTS} > ${PBS_JOBFS}/input-file

mpirun --np $((M * PBS_NCPUS / 48)) \
        --map-by node:PE=${NCPUS} \
        nci-parallel \
        --verbose \
        --input-file ${PBS_JOBFS}/input-file
